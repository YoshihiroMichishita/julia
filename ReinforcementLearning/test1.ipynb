{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_nQ (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux\n",
    "include(\"TwoSpin_env_gpu.jl\")\n",
    "\n",
    "\n",
    "mutable struct agtQ\n",
    "    in_size::Int\n",
    "    out_size::Int\n",
    "    n_dense::Int\n",
    "    ϵ::Float32\n",
    "    γ::Float32\n",
    "    HF_TL::Matrix{Float32}\n",
    "    K_TL::Matrix{Float32}\n",
    "    Kp_TL::Matrix{Float32}\n",
    "end\n",
    "\n",
    "function init_nQ(en::TS_env, n::Int=32, γ0::Float32=0.9, ϵ0::Float32=1.0)\n",
    "    #H_0,V_tのパラメータの数＋K_tの行列＋H_F^a(t)の行列\n",
    "    in_size::Int = en.num_parm + 2*en.HS_size^2 \n",
    "\n",
    "    #K'(t)の行列を出力\n",
    "    out_size::Int = en.HS_size^2\n",
    "\n",
    "    #中間層のニューロンの数\n",
    "    n_dense::Int = n\n",
    "\n",
    "    #乱数発生用のパラメータ\n",
    "    ϵ::Float32 = ϵ0\n",
    "\n",
    "    #割引率\n",
    "    γ::Float32 = γ0\n",
    "\n",
    "    HF_TL = zeros(Float32, en.t_size, en.HS_size^2)\n",
    "    K_TL = zeros(Float32, en.t_size, en.HS_size^2)\n",
    "    Kp_TL = zeros(Float32, en.t_size, en.HS_size^2)\n",
    "\n",
    "    return in_size, out_size, n_dense, ϵ, γ, HF_TL, K_TL, Kp_TL\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nul\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function micro_motion(Kp_t::Vector{Float32}, K_t::Vector{Float32}, en::TS_env, t::Int)\n",
    "    Kp = VtoM(Kp_t,en)\n",
    "    K_t_new::Vector{Float32} = K_t + (2pi/en.t_size/en.Ω) * Kp_t \n",
    "    Kt = VtoM(K_t_new,en)\n",
    "    HF_m = Hermitian(ComplexF32.(exp(1.0im*Kt)*(en.H_0 + en.V_t*sin(2pi*t/en.t_size) - Kp)*exp(-1.0im*Kt)))\n",
    "    HF = MtoV(HF_m, en)\n",
    "    return K_t_new, HF\n",
    "end\n",
    "    \n",
    "    function micro_motion2(Kp_t::Vector{Float32}, K_t::Vector{Float32}, en::TS_env, t::Int)\n",
    "        Kp = VtoM(Kp_t,en)\n",
    "        K_t_new::Vector{Float32} = K_t + (2pi/en.t_size/en.Ω) * Kp_t \n",
    "        Kt = VtoM(K_t_new,en)\n",
    "        HF_m = Hermitian(ComplexF32.(exp(1.0im*Kt)*(en.H_0 + en.V_t*sin(2pi*t/en.t_size) - Kp)*exp(-1.0im*Kt)))\n",
    "        HF = MtoV(HF_m,en)\n",
    "        return HF\n",
    "    end\n",
    "    \n",
    "    \n",
    "    function diff_norm(V::Vector{Float32}, en::TS_env)\n",
    "        M = VtoM(V,en)\n",
    "        e, v = eigen(M)\n",
    "        #n::Float32 = V' * V\n",
    "        n::Float32 = e' * e\n",
    "        #n = sum(e[n]^2 for n in 1:size(e))\n",
    "        return n\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #lossの関数\n",
    "    function loss_fn(en::TS_env, ag::agtQ, t::Int, sw::Int)\n",
    "        l::Float32 = 0.0\n",
    "        for n in 1:(en.t_size-1) \n",
    "            if(n<t)\n",
    "                lt = t-n\n",
    "            elseif(n==t)\n",
    "                lt = en.t_size\n",
    "            else\n",
    "                lt = t-n+en.t_size\n",
    "                if(sw==1) \n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            l -= (ag.γ^(n-1)) * diff_norm(ag.HF_TL[t,:]-ag.HF_TL[lt,:],en)\n",
    "        end\n",
    "        return l\n",
    "    end\n",
    "    \n",
    "    function loss_fn_given(en::TS_env, ag::agtQ,H_t::Vector{Float32}, t::Int, sw::Int)\n",
    "        l::Float32 = 0.0\n",
    "        for n in 1:(en.t_size-1) \n",
    "            if(n<t)\n",
    "                lt = t-n\n",
    "            elseif(n==t)\n",
    "                lt = en.t_size\n",
    "            else\n",
    "                lt = t-n+en.t_size\n",
    "                if(sw==1) \n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            l -= (ag.γ^(n-1)) * diff_norm((H_t-ag.HF_TL[lt,:]),en)\n",
    "        end\n",
    "        return l\n",
    "    end\n",
    "    \n",
    "    function loss_fn_simple(en::TS_env, HF_given::Vector{Float32}, HF_calc::Vector{Float32})\n",
    "        l = -diff_norm(HF_given - HF_calc,en)\n",
    "        return l\n",
    "    end\n",
    "    \n",
    "    function loss_fn_hybrid(en::TS_env, ag::agtQ, HF_given::Vector{Float32}, HF_calc::Vector{Float32}, t::Int)\n",
    "        l::Float32 = 0.0\n",
    "        for n in 1:(en.t_size-1) \n",
    "            if(n<t)\n",
    "                lt = t-n\n",
    "            elseif(n==t)\n",
    "                lt = en.t_size\n",
    "            else\n",
    "                break\n",
    "                #lt = t-n+en.t_size\n",
    "            end\n",
    "            l += ag.ϵ*(ag.γ^(n-1)) * diff_norm((HF_calc-ag.HF_TL[lt,:]),en)/en.t_size\n",
    "        end\n",
    "        #l += diff_norm(HF_given - HF_calc,en)/en.t_size\n",
    "        return l\n",
    "    end\n",
    "    \n",
    "    function loss_calc0(model0, en::TS_env, ag::agtQ, t::Int, HF_given::Vector{Float32})\n",
    "        if(t==1)\n",
    "            tt=en.t_size\n",
    "        else\n",
    "            tt=t-1\n",
    "        end\n",
    "        p = [en.Ω, en.ξ*sin(2pi*t/en.t_size), en.Jz, en.Jx, en.hz]\n",
    "        x = vcat([p, ag.K_TL[tt,:], ag.HF_TL[tt,:]]...)\n",
    "        Kp = model0(x)\n",
    "        \n",
    "        #ag.K_TL[t,:] += Kp\n",
    "        HF_calc = micro_motion2(Kp, ag.K_TL[tt,:],en,t)\n",
    "        l = -loss_fn_simple(en, HF_given, HF_calc)\n",
    "        #l = Kp' * Kp\n",
    "        return l \n",
    "    end\n",
    "    \n",
    "    function loss_calc(model0, en::TS_env, ag::agtQ, t::Int, it::Int)\n",
    "        if(t==1)\n",
    "            tt=en.t_size\n",
    "        else\n",
    "            tt=t-1\n",
    "        end\n",
    "        p = [en.Ω, en.ξ*sin(2pi*t/en.t_size), en.Jz, en.Jx, en.hz]\n",
    "        x = vcat([p, ag.K_TL[tt,:], ag.HF_TL[tt,:]]...)\n",
    "        Kp = model0(x)\n",
    "        \n",
    "        #ag.K_TL[t,:] += Kp\n",
    "        HF_t = micro_motion2(Kp, ag.K_TL[tt,:],en,t)\n",
    "        l = -loss_fn_given(en, ag, HF_t, t, it)\n",
    "        #l = Kp' * Kp\n",
    "        return l \n",
    "    end\n",
    "    \n",
    "    function loss_calc_hyb(model0, en::TS_env, ag::agtQ, HF_given::Vector{Float32})\n",
    "        l::Float32 = 0.0\n",
    "        kp_sum = zeros(Float32, en.HS_size^2)\n",
    "        for t in 1:en.t_size\n",
    "            if(t==1)\n",
    "                tt=en.t_size\n",
    "            else\n",
    "                tt=t-1\n",
    "            end\n",
    "            p = [en.Ω, Float32(en.ξ*sin(2pi*t/en.t_size)), en.Jz, en.Jx, en.hz]\n",
    "            x = vcat([p, ag.K_TL[tt,:], ag.Kp_TL[tt,:]]...)\n",
    "            xg = gpu(x)\n",
    "            Kp = Array{Float32}(model0(xg))\n",
    "            kp_sum += Kp\n",
    "            #ag.K_TL[t,:] += Kp\n",
    "            HF_calc = micro_motion2(Kp, ag.K_TL[tt,:],en,t)\n",
    "            l += loss_fn_hybrid(en,ag, HF_given, HF_calc,t)\n",
    "            #l += ag.ϵ^2*diff_norm(kp_sum,en)/en.t_size\n",
    "            #l += diff_norm(kp_sum,en)/en.t_size\n",
    "            #l += ag.γ^(5*(en.t_size/2 - abs(en.t_size/2-t))) * diff_norm(ag.K_TL[t,:],en)\n",
    "            #if(t==t_size)\n",
    "            #    l += diff_norm(HF_calc-ag.HF_TL[1,:],en)\n",
    "            #end\n",
    "        end\n",
    "        l += diff_norm(kp_sum,en)/en.t_size\n",
    "        return l \n",
    "    end\n",
    "    \n",
    "    function loss_calc_hyb!(model0, en::TS_env, ag::agtQ, HF_given::Vector{Float32})\n",
    "        l::Float32 = 0.0\n",
    "        kp_sum = zeros(Float32, en.HS_size^2)\n",
    "        for t in 1:en.t_size\n",
    "            if(t==1)\n",
    "                tt=en.t_size\n",
    "            else\n",
    "                tt=t-1\n",
    "            end\n",
    "            p = [en.Ω, en.ξ*sin(2pi*t/en.t_size), en.Jz, en.Jx, en.hz]\n",
    "            x = vcat([p, ag.K_TL[tt,:], ag.Kp_TL[tt,:]]...)\n",
    "            xg = gpu(x)\n",
    "            ag.Kp_TL[t,:] = Array{Float32}(model0(xg))\n",
    "            kp_sum += ag.Kp_TL[t,:]\n",
    "            #ag.K_TL[t,:] += Kp\n",
    "            ag.K_TL[t,:], ag.HF_TL[t,:] = micro_motion(ag.Kp_TL[t,:], ag.K_TL[tt,:],en,t)\n",
    "            l += loss_fn_hybrid(en,ag, HF_given, ag.HF_TL[t,:],t)\n",
    "            #l += diff_norm(kp_sum,en)/en.t_size\n",
    "            #l += ag.γ^(5*(en.t_size/2 - abs(en.t_size/2-t))) * diff_norm(ag.K_TL[t,:],en)\n",
    "            #l += ag.γ^(5*(en.t_size - t)) * diff_norm(ag.K_TL[t,:],en)\n",
    "            #if(t==t_size)\n",
    "            #    l += diff_norm(ag.HF_TL[t,:]-ag.HF_TL[1,:],en)\n",
    "            #end\n",
    "        end\n",
    "        l += diff_norm(kp_sum,en)\n",
    "        return l, kp_sum/en.t_size\n",
    "    end\n",
    "    \n",
    "    function loss_calc!(model0, en::TS_env, ag::agtQ, t::Int, HF_given::Vector{Float32})\n",
    "        if(t==1)\n",
    "            tt=en.t_size\n",
    "        else\n",
    "            tt=t-1\n",
    "        end\n",
    "        p = [en.Ω, en.ξ*sin(2pi*t/en.t_size), en.Jz, en.Jx, en.hz]\n",
    "        x = vcat([p, ag.K_TL[tt,:], ag.HF_TL[tt,:]]...)\n",
    "        Kp = model0(x)\n",
    "        \n",
    "        #ag.K_TL[t,:] += Kp\n",
    "        ag.K_TL[t,:], ag.HF_TL[t,:] = micro_motion(Kp, ag.K_TL[tt,:],en,t)\n",
    "        l = -loss_fn_simple(en, HF_given, ag.HF_TL[t,:])\n",
    "        #l = -loss_fn(en, ag, t, it)\n",
    "        #l = Kp' * Kp\n",
    "        return l \n",
    "    end\n",
    "    \n",
    "    function updata_KpK!(en::TS_env, ag::agtQ, Kp_av::Vector{Float32})\n",
    "        for t in 1:en.t_size\n",
    "            ag.Kp_TL[t,:] = ag.Kp_TL[t,:] - Kp_av\n",
    "            ag.K_TL[t,:] = ag.K_TL[t,:] - t*Kp_av\n",
    "            ag.HF_TL[t,:] = micro_motion2(ag.Kp_TL[t,:], ag.K_TL[t,:], en, t)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    function init_HF(en::TS_env)\n",
    "        jp = en.Jz + en.hz\n",
    "        jm = en.Jz - en.hz\n",
    "        VHmHV::Vector{Float32} = 4*en.ξ*[0.0, 0.0, jp, 0.0, jp, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -jm, 0.0, 0.0, -jm, 0.0]\n",
    "        init = MtoV(en.H_0, en) + VHmHV\n",
    "        return init\n",
    "    end\n",
    "    \n",
    "    using DataFrames\n",
    "    using CSV\n",
    "    using BSON: @save\n",
    "    using Plots\n",
    "    ENV[\"GKSwstype\"]=\"nul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en = TS_env(init_env(Int(100), Float32(10.0), Float32(0.2), Float32(1.0), Float32(0.7), Float32(0.5))...)\n",
    "\n",
    "ag = agtQ(init_nQ(en, Int(512), Float32(0.95), Float32(3.0))...)\n",
    "\n",
    "    #二次の高周波展開で初期値を代入\n",
    "ag.HF_TL[en.t_size,:] = init_HF(en)\n",
    "ag.K_TL[en.t_size,:] = zeros(Float32, en.HS_size^2)\n",
    "    #-MtoV(en.V_t, en)/en.Ω\n",
    "\n",
    "    #model = Chain(Dense(ag.in_size, ag.n_dense, tanh), Dense(ag.n_dense, ag.n_dense, tanh), Dense(ag.n_dense, ag.n_dense, tanh), Dense(ag.n_dense, ag.out_size))\n",
    "model = Chain(Dense(ag.in_size, ag.n_dense, tanh), Dense(ag.n_dense, ag.n_dense, tanh), Dense(ag.n_dense, ag.out_size))\n",
    "model_g = fmap(cu, model)\n",
    "    #model = Chain(Dense(zeros(Float32, ag.n_dense, ag.in_size), zeros(Float32, ag.n_dense), tanh), Dense(zeros(Float32, ag.n_dense, ag.n_dense), zeros(Float32, ag.n_dense), tanh), Dense(zeros(Float32, ag.out_size, ag.n_dense), zeros(Float32, ag.out_size)))\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_calc Finish!\n"
     ]
    }
   ],
   "source": [
    "HF_it = zeros(Float32, en.HS_size^2) \n",
    "\n",
    "for t_step in 1:en.t_size\n",
    "        if(t_step==1)\n",
    "                tt=en.t_size\n",
    "        else\n",
    "                tt=t_step-1\n",
    "        end\n",
    "        p = [en.Ω, en.ξ*sin(2pi*t_step/en.t_size), en.Jz, en.Jx, en.hz]\n",
    "        x = vcat([p, ag.K_TL[tt,:], ag.Kp_TL[tt,:]]...)\n",
    "        xg = gpu(x)\n",
    "        ag.Kp_TL[t_step,:] = Array(model_g(xg))\n",
    "        ag.K_TL[t_step,:], ag.HF_TL[t_step,:] = micro_motion(ag.Kp_TL[t_step,:], ag.K_TL[tt,:],en,t_step)\n",
    "        #ag.K_TL[t,:] += Kp\n",
    "        HF_it += ag.HF_TL[t_step,:]/en.t_size\n",
    "end\n",
    "println(\"HF_calc Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_calc Finish!\n"
     ]
    }
   ],
   "source": [
    "HF_it = zeros(Float32, en.HS_size^2) \n",
    "\n",
    "for t_step in 1:en.t_size\n",
    "        if(t_step==1)\n",
    "                tt=en.t_size\n",
    "        else\n",
    "                tt=t_step-1\n",
    "        end\n",
    "        p = [en.Ω, en.ξ*sin(2pi*t_step/en.t_size), en.Jz, en.Jx, en.hz]\n",
    "        x = vcat([p, ag.K_TL[tt,:], ag.Kp_TL[tt,:]]...)\n",
    "        #xg = gpu(x)\n",
    "        ag.Kp_TL[t_step,:] = model(x)\n",
    "        ag.K_TL[t_step,:], ag.HF_TL[t_step,:] = micro_motion(ag.Kp_TL[t_step,:], ag.K_TL[tt,:],en,t_step)\n",
    "        #ag.K_TL[t,:] += Kp\n",
    "        HF_it += ag.HF_TL[t_step,:]/en.t_size\n",
    "end\n",
    "println(\"HF_calc Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "GPUCompiler.KernelError",
     "evalue": "GPU compilation of kernel #broadcast_kernel#17(CUDA.CuKernelContext, CuDeviceMatrix{Float32, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(*), Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(*), Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}} which is not isbits.\n    .1 is of type Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}} which is not isbits.\n      .x is of type Vector{Float32} which is not isbits.\n\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel #broadcast_kernel#17(CUDA.CuKernelContext, CuDeviceMatrix{Float32, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(*), Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, Int64) failed\n",
      "KernelError: passing and using non-bitstype argument\n",
      "\n",
      "Argument 4 to your kernel function is of type Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(*), Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, which is not isbits:\n",
      "  .args is of type Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}} which is not isbits.\n",
      "    .1 is of type Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}} which is not isbits.\n",
      "      .x is of type Vector{Float32} which is not isbits.\n",
      "\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] check_invocation(job::GPUCompiler.CompilerJob)\n",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/validation.jl:88\n",
      "  [2] macro expansion\n",
      "    @ ~/.julia/packages/GPUCompiler/07qaN/src/driver.jl:417 [inlined]\n",
      "  [3] macro expansion\n",
      "    @ ~/.julia/packages/TimerOutputs/4yHI4/src/TimerOutput.jl:253 [inlined]\n",
      "  [4] macro expansion\n",
      "    @ ~/.julia/packages/GPUCompiler/07qaN/src/driver.jl:416 [inlined]\n",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)\n",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/utils.jl:68\n",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context)\n",
      "    @ CUDA ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:354\n",
      "  [7] #224\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:347 [inlined]\n",
      "  [8] JuliaContext(f::CUDA.var\"#224#225\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#17\", Tuple{CUDA.CuKernelContext, CuDeviceMatrix{Float32, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(*), Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, Int64}}}})\n",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/driver.jl:76\n",
      "  [9] cufunction_compile(job::GPUCompiler.CompilerJob)\n",
      "    @ CUDA ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:346\n",
      " [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))\n",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/cache.jl:90\n",
      " [11] cufunction(f::GPUArrays.var\"#broadcast_kernel#17\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceMatrix{Float32, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(*), Tuple{Base.Broadcast.Extruded{Vector{Float32}, Tuple{Bool}, Tuple{Int64}}, Base.Broadcast.Extruded{Adjoint{Float32, CuDeviceVector{Float32, 1}}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, Int64}}; name::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n",
      "    @ CUDA ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:299\n",
      " [12] cufunction\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:292 [inlined]\n",
      " [13] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:102 [inlined]\n",
      " [14] #launch_heuristic#248\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/gpuarrays.jl:17 [inlined]\n",
      " [15] _copyto!\n",
      "    @ ~/.julia/packages/GPUArrays/fqD8z/src/host/broadcast.jl:63 [inlined]\n",
      " [16] copyto!\n",
      "    @ ~/.julia/packages/GPUArrays/fqD8z/src/host/broadcast.jl:46 [inlined]\n",
      " [17] copy\n",
      "    @ ~/.julia/packages/GPUArrays/fqD8z/src/host/broadcast.jl:37 [inlined]\n",
      " [18] materialize\n",
      "    @ ./broadcast.jl:860 [inlined]\n",
      " [19] broadcast(::typeof(*), ::Vector{Float32}, ::Adjoint{Float32, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}})\n",
      "    @ Base.Broadcast ./broadcast.jl:798\n",
      " [20] *\n",
      "    @ /opt/julia-1.8.2/share/julia/stdlib/v1.8/LinearAlgebra/src/adjtrans.jl:315 [inlined]\n",
      " [21] #1466\n",
      "    @ ~/.julia/packages/ChainRules/KVV0e/src/rulesets/Base/arraymath.jl:56 [inlined]\n",
      " [22] unthunk\n",
      "    @ ~/.julia/packages/ChainRulesCore/C73ay/src/tangent_types/thunks.jl:204 [inlined]\n",
      " [23] unthunk\n",
      "    @ ~/.julia/packages/ChainRulesCore/C73ay/src/tangent_types/thunks.jl:237 [inlined]\n",
      " [24] wrap_chainrules_output\n",
      "    @ ~/.julia/packages/Zygote/dABKa/src/compiler/chainrules.jl:105 [inlined]\n",
      " [25] map\n",
      "    @ ./tuple.jl:223 [inlined]\n",
      " [26] wrap_chainrules_output\n",
      "    @ ~/.julia/packages/Zygote/dABKa/src/compiler/chainrules.jl:106 [inlined]\n",
      " [27] ZBack\n",
      "    @ ~/.julia/packages/Zygote/dABKa/src/compiler/chainrules.jl:206 [inlined]\n",
      " [28] Pullback\n",
      "    @ ~/.julia/packages/Flux/4k0Ls/src/layers/basic.jl:172 [inlined]\n",
      " [29] macro expansion\n",
      "    @ ~/.julia/packages/Flux/4k0Ls/src/layers/basic.jl:53 [inlined]\n",
      " [30] Pullback\n",
      "    @ ~/.julia/packages/Flux/4k0Ls/src/layers/basic.jl:53 [inlined]\n",
      " [31] (::typeof(∂(_applychain)))(Δ::Vector{Float32})\n",
      "    @ Zygote ~/.julia/packages/Zygote/dABKa/src/compiler/interface2.jl:0\n",
      " [32] Pullback\n",
      "    @ ~/.julia/packages/Flux/4k0Ls/src/layers/basic.jl:51 [inlined]\n",
      " [33] (::typeof(∂(λ)))(Δ::Vector{Float32})\n",
      "    @ Zygote ~/.julia/packages/Zygote/dABKa/src/compiler/interface2.jl:0\n",
      " [34] Pullback\n",
      "    @ ~/Documents/Codes/julia/ReinforcementLearning/test1.ipynb:135 [inlined]\n",
      " [35] (::typeof(∂(loss_calc_hyb)))(Δ::Float32)\n",
      "    @ Zygote ~/.julia/packages/Zygote/dABKa/src/compiler/interface2.jl:0\n",
      " [36] Pullback\n",
      "    @ ~/Documents/Codes/julia/ReinforcementLearning/test1.ipynb:2 [inlined]\n",
      " [37] (::typeof(∂(#15)))(Δ::Float32)\n",
      "    @ Zygote ~/.julia/packages/Zygote/dABKa/src/compiler/interface2.jl:0\n",
      " [38] (::Zygote.var\"#99#100\"{Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, typeof(∂(#15)), Zygote.Context{true}})(Δ::Float32)\n",
      "    @ Zygote ~/.julia/packages/Zygote/dABKa/src/compiler/interface.jl:378\n",
      " [39] gradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/dABKa/src/compiler/interface.jl:97\n",
      " [40] top-level scope\n",
      "    @ ~/Documents/Codes/julia/ReinforcementLearning/test1.ipynb:1"
     ]
    }
   ],
   "source": [
    "grads = Flux.gradient(Flux.params(model_g)) do\n",
    "        loss_calc_hyb(model_g, en, ag, HF_it)\n",
    "        #loss_calc0(model, en, ag, t_step, HF_it)\n",
    "        #loss_t(model, en, ag, t_step, it)\n",
    "        #loss_t!(model, en, ag, t_step, it)\n",
    "end\n",
    "Flux.Optimise.update!(opt, Flux.params(model_g), grads)\n",
    "println(\"Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
